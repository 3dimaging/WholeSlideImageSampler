{
    "docs": [
        {
            "location": "/",
            "text": "Whole-Slide-Image sampler\n\n\nGitHub\n, \nDocs\n. (Docs autogenerated with \npydoc-markdown\n. \nTo do: Images not working on docs.\n)\n\n\nThis respository aims to develop a tool for sampling from Whole-Slide-Images (WSIs) in an efficient manner. By sampling we mean producing batches of patches which can then be fed to e.g. machine learning algorithms. This should ideally be achieved without storing all the patches on disk (waste of storage). It aims to work with all WSIs that can be read by \nopenslide\n. Sample data for tests is available \nhere\n (images from the opensource \nCamelyon 16\n dataset).\n\n\nAssumptions\n\n\n\n\nYou have WSIs in a format readable by \nopenslide\n.\n\n\nYou may also have multi-resolution-image annotation files, such as those exportable by the (excellent) slide annotation tool \nASAP\n. For example, see the folder 'annotation' in the \nsample data\n. The annotations can be multi-class. \nThe multi-resolution level structure should be the same for both the main WSI and also for the annotation\n.\n\n\n\n\nUsage\n\n\nHave a look at the tests in tests folder\n\n\nSampling is achieved through a \nSingle_Sampler\n object, which is implemented in the module \nsingle_sampler.py\n. We first build an object:\n\n\nsampler = single_sampler.Single_Sampler(wsi_file, background_dir, annotation_dir, level0=40.)\n\n\nwhere\n\n\n\n\nwsi_file\n : a string path to a WSI file\n\n\nbackground_dir\n : a (string) directory for background masks. If this directory does not exist or no mask for this WSI is found then a background is generated and saved to this directory (creating the directory if necessary).\n\n\nannotation_dir\n : a (string) directory where annotations are stored (or \nNone\n). We automatically look through this directory and assign the correct WSI annotation to the sampler if found, else assign no annotation.\n\n\nlevel0\n : the WSI (and annotation) resolution at 'level 0'.\n\n\n\n\nWe then prepare for sampling with something like:\n\n\nsampler.prepare_sampling(desired_downsampling, patchsize)\n\n\nwhere\n\n\n\n\ndesired_downsampling\n : the desired downsampling. e.g. for a WSI with level0 at 40X a downsampling of 4 gives patches at 10X.\n\n\npatchsize\n : patches sampled at size (patchsize x patchsize).\n\n\n\n\nAt this point we are ready to sample patches with:\n\n\nsampler.sample_patches(max_per_class, savedir, verbose=0)\n\n\nwhere\n\n\n\n\nmax_per_class\n : the maximum number of patches to get per class\n\n\nsavedir\n : location to save output patchframe\n\n\nverbose\n : (bool) report number of rejected patches?\n\n\n\n\nThe sampled patches are saved to \nsavedir\n in the form of a \npatchframe\n. A patchframe is defined as a datastructure containing coordinates of patches as well as metadata like the patch class and also the patch parent WSI, size, level. This is implemented with a \npandas\n pd.DataFrame. Note that the patches themselves are not stored! Using this database of patches we can then pass patches to a machine learning algorithm.\n\n\ne.g.\n\n\npatchframe head:\n       w       h class                                             parent level size\n0  30900  119768     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n1  78691  170619     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n2  67651  158458     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n3  65468  156771     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n4  40402  115702     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n\n\n\n\nBackground generation\n\n\nThe background mask is stored as a downsampled, boolean numpy array where True denotes tissue and False denotes background. This is generated from the WSI using Otsu thresholding on the saturation channel followed by morphological operations. This is inspired by \nthis paper\n, which achieved top results in Camelyon 16 contest. The generated background mask can be visualized using e.g.\n\n\nsampler.save_background_visualization(savedir)\n.\n\n\nleft:\n* Normal slides, \nright:*\n Cancer slides\n\n\n\n\n\n\nAnnotation viewing\n\n\nMulti-resolution annotations are best viewed with ASAP but if you want to visualize here we can with e.g.\n\n\nsampler.save_annotation_visualization(savedir)\n\n\nAnnotations on the two cancerous slides shown above\n\n\n\n\nPatch viewing\n\n\nWe might want to inspect the patches listed in a patchframe. To do this we can save them to disk with e.g.\n\n\nutils.save_patchframe_patches(patchframe)\n (utils module)\n\n\nleft:\n* two class 0 (Normal) patches, \nright:*\n two class 1 (cancer) patches (256x256 @ 10X).",
            "title": "Home"
        },
        {
            "location": "/#whole-slide-image-sampler",
            "text": "GitHub ,  Docs . (Docs autogenerated with  pydoc-markdown .  To do: Images not working on docs. )  This respository aims to develop a tool for sampling from Whole-Slide-Images (WSIs) in an efficient manner. By sampling we mean producing batches of patches which can then be fed to e.g. machine learning algorithms. This should ideally be achieved without storing all the patches on disk (waste of storage). It aims to work with all WSIs that can be read by  openslide . Sample data for tests is available  here  (images from the opensource  Camelyon 16  dataset).",
            "title": "Whole-Slide-Image sampler"
        },
        {
            "location": "/#assumptions",
            "text": "You have WSIs in a format readable by  openslide .  You may also have multi-resolution-image annotation files, such as those exportable by the (excellent) slide annotation tool  ASAP . For example, see the folder 'annotation' in the  sample data . The annotations can be multi-class.  The multi-resolution level structure should be the same for both the main WSI and also for the annotation .",
            "title": "Assumptions"
        },
        {
            "location": "/#usage",
            "text": "Have a look at the tests in tests folder  Sampling is achieved through a  Single_Sampler  object, which is implemented in the module  single_sampler.py . We first build an object:  sampler = single_sampler.Single_Sampler(wsi_file, background_dir, annotation_dir, level0=40.)  where   wsi_file  : a string path to a WSI file  background_dir  : a (string) directory for background masks. If this directory does not exist or no mask for this WSI is found then a background is generated and saved to this directory (creating the directory if necessary).  annotation_dir  : a (string) directory where annotations are stored (or  None ). We automatically look through this directory and assign the correct WSI annotation to the sampler if found, else assign no annotation.  level0  : the WSI (and annotation) resolution at 'level 0'.   We then prepare for sampling with something like:  sampler.prepare_sampling(desired_downsampling, patchsize)  where   desired_downsampling  : the desired downsampling. e.g. for a WSI with level0 at 40X a downsampling of 4 gives patches at 10X.  patchsize  : patches sampled at size (patchsize x patchsize).   At this point we are ready to sample patches with:  sampler.sample_patches(max_per_class, savedir, verbose=0)  where   max_per_class  : the maximum number of patches to get per class  savedir  : location to save output patchframe  verbose  : (bool) report number of rejected patches?   The sampled patches are saved to  savedir  in the form of a  patchframe . A patchframe is defined as a datastructure containing coordinates of patches as well as metadata like the patch class and also the patch parent WSI, size, level. This is implemented with a  pandas  pd.DataFrame. Note that the patches themselves are not stored! Using this database of patches we can then pass patches to a machine learning algorithm.  e.g.  patchframe head:\n       w       h class                                             parent level size\n0  30900  119768     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n1  78691  170619     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n2  67651  158458     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n3  65468  156771     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256\n4  40402  115702     0  /home/peter/Dropbox/publish-final/WSI_sampler_...     2  256",
            "title": "Usage"
        },
        {
            "location": "/#background-generation",
            "text": "The background mask is stored as a downsampled, boolean numpy array where True denotes tissue and False denotes background. This is generated from the WSI using Otsu thresholding on the saturation channel followed by morphological operations. This is inspired by  this paper , which achieved top results in Camelyon 16 contest. The generated background mask can be visualized using e.g.  sampler.save_background_visualization(savedir) .  left: * Normal slides,  right:*  Cancer slides",
            "title": "Background generation"
        },
        {
            "location": "/#annotation-viewing",
            "text": "Multi-resolution annotations are best viewed with ASAP but if you want to visualize here we can with e.g.  sampler.save_annotation_visualization(savedir)  Annotations on the two cancerous slides shown above",
            "title": "Annotation viewing"
        },
        {
            "location": "/#patch-viewing",
            "text": "We might want to inspect the patches listed in a patchframe. To do this we can save them to disk with e.g.  utils.save_patchframe_patches(patchframe)  (utils module)  left: * two class 0 (Normal) patches,  right:*  two class 1 (cancer) patches (256x256 @ 10X).",
            "title": "Patch viewing"
        },
        {
            "location": "/single_slide/",
            "text": "modules.single_sampler\n\n\n\nsingle_sampler module\n\n\nSingle_Sampler\n\n\n\nSingle_Sampler(self, wsi_file, background_dir, annotation_dir, level0=40.0)\n\n\n\n\nParameters\n\n\n\n\n:param wsi_file\n: path to a WSI file\n\n\n:param background_dir\n: directory where we do/will store background masks (NumpyBackground objects)\n\n\n:param annotation_dir\n: directory where we keep annotations\n\n\n:param level0\n: resolution at level 0 (usually 40X)\n\n\n\n\nclass_c_patch_i\n\n\n\nSingle_Sampler.class_c_patch_i(self, c, i)\n\n\n\n\nTry and get the ith patch of class c. If we reject return (None, None).\n\nParameters\n\n\n\n\n:param c\n: class\n\n\n:param i\n: index\n\n\n\n\nReturns\n\n\n:return\n: (patch, info_dict) or (None, None) if we reject patch.\n\n\nsave_annotation_visualization\n\n\n\nSingle_Sampler.save_annotation_visualization(self, savedir='/home/peter/projects_/slide_loader/docsbuilder')\n\n\n\n\nSave a visualization of the annotation\n\nParameters\n\n\n\n\n:param savedir\n: where to save to\n\n\n\n\nsave_background_visualization\n\n\n\nSingle_Sampler.save_background_visualization(self, savedir='/home/peter/projects_/slide_loader/docsbuilder')\n\n\n\n\nSave a visualization of the background mask\n\nParameters\n\n\n\n\n:param savedir\n: where to save to\n\n\n\n\nsample_patches\n\n\n\nSingle_Sampler.sample_patches(self, max_per_class=100, savedir='/home/peter/projects_/slide_loader/docsbuilder', verbose=0)\n\n\n\n\nSample patches and save in a patchframe\n\nParameters\n\n\n\n\n:param max_per_class\n: maximum number of patches per class\n\n\n:param savedir\n: where to save patchframe\n\n\n:param verbose\n: report number of rejected patches?\n\n\n\n\nlevel_converter\n\n\n\nSingle_Sampler.level_converter(self, x, lvl_in, lvl_out)\n\n\n\n\nConvert a length/coordinate 'x' from lvl_in to lvl_out\n\nParameters\n\n\n\n\n:param x\n: a length/coordinate\n\n\n:param lvl_in\n: level to convert from\n\n\n:param lvl_out\n: level to convert to\n\n\n\n\nReturns\n\n\n:return\n: New length/coordinate\n\n\nget_classes_and_seeds\n\n\n\nSingle_Sampler.get_classes_and_seeds(self)\n\n\n\n\nGet classes and approximate coordinates to 'seed' the patch sampling process\n\n\nprepare_sampling\n\n\n\nSingle_Sampler.prepare_sampling(self, desired_downsampling, patchsize)\n\n\n\n\nParameters\n\n\n\n\n:param desired_downsampling\n: the desired downsampling. E.g. if level 0 is 40X then a downsampling of 4 is 10X.\n\n\n:param patchsize\n: sample patches of size patchsize x patchsize\n\n\n\n\npickle_NumpyBackground\n\n\n\nSingle_Sampler.pickle_NumpyBackground(self, savedir='/home/peter/projects_/slide_loader/docsbuilder')\n\n\n\n\nSave (pickle) the NumpyBackground object\n\nParameters\n\n\n\n\n:param savedir\n: where to save to\n\n\n\n\nvalidate_NumpyBackground\n\n\n\nSingle_Sampler.validate_NumpyBackground(self)\n\n\n\n\nMini check on NumpyBackground object\n\n\nNumpyBackround\n\n\n\nNumpyBackround(self, parent_wsi, approx_downsampling, threshold=20)\n\n\n\n\nAn object for storing the background mask",
            "title": "single_sampler"
        },
        {
            "location": "/utils/",
            "text": "modules.utils\n\n\n\nutils module\n\n\ngenerate_background_mask\n\n\n\ngenerate_background_mask(wsi, level)\n\n\n\n\nGenerate a background mask.\nThis is achieved by otsu thresholding on the saturation channel followed by morphological closing and opening to remove noise.\n\nParameters\n\n\n\n\n:param wsi\n:\n\n\n:param level\n:\n\n\n:return\n:\n\n\n\n\nget_patch_from_info_dict\n\n\n\nget_patch_from_info_dict(info)\n\n\n\n\nGet a patch from an info dict\n\nParameters\n\n\n\n\n:param info\n: info dict\n\nReturns\n\n\n\n\n:return\n: patch (PIL image)\n\n\nsave_patchframe_patches\n\n\n\nsave_patchframe_patches(input, save_dir='/home/peter/projects_/slide_loader/docsbuilder/patches')\n\n\n\n\nSave patches in a patchframe to disk for visualization\n\nParameters\n\n\n\n\n:param input\n: patchframe or pickled patchframe\n\n\n:param save_dir\n: where to save to\n\n\n\n\nstring_in_directory\n\n\n\nstring_in_directory(s, dir)\n\n\n\n\nIs a string in a filename in the given directory?\n\nParameters\n\n\n\n\n:param s\n: string\n\n\n:param dir\n: directory\n\nReturns\n\n\n\n\n:return\n: (bool, string)\n\n\nget_level\n\n\n\nget_level(OpenSlide, desired_downsampling, threshold)\n\n\n\n\nGet the level for a desired downsampling. Threshold controls how close true and desired downsampling must be.\n\nParameters\n\n\n\n\n:param OpenSlide\n:\n\n\n:param desired_downsampling\n:\n\n\n:param threshold\n:\n\nReturns\n\n\n\n\n:return\n: level",
            "title": "utils"
        }
    ]
}